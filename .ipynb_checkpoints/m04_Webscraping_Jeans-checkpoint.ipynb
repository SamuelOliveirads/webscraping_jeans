{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eaa1ac2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Questão de Negócio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd05c7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A empresa Star Jeans! Eduardo e Marcelo são dois brasileiros, amigos e sócios de empreendimento. \n",
    "\n",
    "Depois de vários negócio bem sucedidos, eles estão planejando entrar no mercado de moda\n",
    "dos USA como um modelo de negócio do tipo E-commerce.\n",
    "\n",
    "A idéia inicial é entrar no mercado com apenas um produto e para um público específico, no caso\n",
    "o produto seria calças Jenas para o público masculino. O objetivo é manter o custo de operação\n",
    "baixo e escalar a medida que forem conseguindo clientes.\n",
    "\n",
    "Porém, mesmo com o produto de entrada e a audiência definidos, os dois sócios não tem experiência\n",
    "nesse mercado de moda e portanto não sabem definir coisas básicas como preço, o tipo de calça e\n",
    "o material para a fabricação de cada peça.\n",
    "\n",
    "Assim, os dois sócios contrataram uma consultoria de Ciência de Dados para responder as seguintes\n",
    "perguntas: \n",
    "1. Qual o melhor preço de venda para as calças? \n",
    "2. Quantos tipos de calças e suas cores para o produto inicial? \n",
    "3. Quais as matérias-prima necessárias para confeccionar as calças?\n",
    "\n",
    "As principais concorrentes da empresa Start Jeans são as americadas H&M e Macys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a60f16",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 0.0 Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0774dc6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-10T23:26:19.578907Z",
     "start_time": "2022-08-10T23:26:17.368273Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "import inflection\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ac345",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 0.1 Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b2bba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-10T23:26:19.594884Z",
     "start_time": "2022-08-10T23:26:19.580891Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def jupyter_settings():\n",
    "    %matplotlib inline\n",
    "    plt.style.use('bmh')\n",
    "    plt.rcParams['figure.figsize'] = [25,12]\n",
    "    plt.rcParams['font.size'] = 24\n",
    "    plt.rcParams['figure.dpi'] = 100\n",
    "    \n",
    "    display( HTML( '<style>.container{width:100% !important; }</style>'))\n",
    "    pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "    \n",
    "    # ignora future warnings\n",
    "    #warnings.filterwarnings('ignore')\n",
    "    \n",
    "    sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "479a6ce8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-10T23:26:19.735099Z",
     "start_time": "2022-08-10T23:26:19.599882Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container{width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jupyter_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53bdf7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1.0 Organização do ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8614dd80",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.1 Extração de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a32d5a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-05T14:45:10.215207Z",
     "start_time": "2022-08-05T14:45:07.816196Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#------------------ Make URL with all products -----------------------------\n",
    "# URL\n",
    "url = 'https://www2.hm.com/en_us/men/products/jeans.html'\n",
    "\n",
    "# parameters\n",
    "headers = {'User-Agent': 'Mozilla/5.0 {Macintosh; Intel Mac Os X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36}'}\n",
    "\n",
    "# make request\n",
    "page = requests.get(url, headers=headers)\n",
    "\n",
    "# Instance Beautiful Soup\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "# collect all products available\n",
    "total_item = soup.find_all('h2', class_='load-more-heading')[0].get('data-total')\n",
    "\n",
    "# detect total page numbers\n",
    "page_number = np.ceil(int(total_item)/36)\n",
    "\n",
    "# create link from total of page numbers\n",
    "url = url + '?page-size=' + str(int(page_number*36))\n",
    "\n",
    "\n",
    "#------------------ New Request with all products ----------------\n",
    "# make request\n",
    "page = requests.get(url, headers=headers)\n",
    "\n",
    "# Instance Beautiful Soup\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "# ---------------- Product Data ---------------------------------\n",
    "\n",
    "# extract all products\n",
    "products = soup.find('ul', class_='products-listing small')\n",
    "\n",
    "product_list = products.find_all('article', class_='hm-product-item')\n",
    "\n",
    "# product id\n",
    "product_id = [p.get('data-articlecode') for p in product_list]\n",
    "\n",
    "# product_category\n",
    "product_category = [p.get('data-category') for p in product_list]\n",
    "\n",
    "# product name\n",
    "product_list = products.find_all('a', class_='link')\n",
    "product_name = [p.get_text() for p in product_list]\n",
    "\n",
    "# price\n",
    "product_list = products.find_all('span', class_='price regular')\n",
    "product_price = [p.get_text() for p in product_list]\n",
    "\n",
    "# merge scrapy into data frame\n",
    "data = pd.DataFrame([product_id, product_category, product_name, product_price]).T\n",
    "data.columns = ['product_id', 'product_category', 'product_name', 'product_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a986f6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.2 Extração de dados por produtos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6f44557b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-05T17:14:19.177639Z",
     "start_time": "2022-08-05T17:13:08.031934Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product: https://www2.hm.com/en_us/productpage.0971061002.html\n",
      "color: https://www2.hm.com/en_us/productpage.0971061001.html\n",
      "color: https://www2.hm.com/en_us/productpage.0971061002.html\n",
      "color: https://www2.hm.com/en_us/productpage.0971061004.html\n",
      "color: https://www2.hm.com/en_us/productpage.0971061005.html\n",
      "color: https://www2.hm.com/en_us/productpage.0971061006.html\n",
      "product: https://www2.hm.com/en_us/productpage.0985159007.html\n",
      "color: https://www2.hm.com/en_us/productpage.0985159001.html\n",
      "color: https://www2.hm.com/en_us/productpage.0985159002.html\n",
      "product: https://www2.hm.com/en_us/productpage.0985159001.html\n",
      "color: https://www2.hm.com/en_us/productpage.0985159001.html\n",
      "color: https://www2.hm.com/en_us/productpage.0985159002.html\n",
      "color: https://www2.hm.com/en_us/productpage.0985159003.html\n",
      "color: https://www2.hm.com/en_us/productpage.0985159004.html\n",
      "color: https://www2.hm.com/en_us/productpage.0985159005.html\n",
      "product: https://www2.hm.com/en_us/productpage.1024256001.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256001.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256002.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256003.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256004.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256005.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256006.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256007.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256008.html\n",
      "product: https://www2.hm.com/en_us/productpage.1008549001.html\n",
      "color: https://www2.hm.com/en_us/productpage.1008549001.html\n",
      "color: https://www2.hm.com/en_us/productpage.1008549002.html\n",
      "color: https://www2.hm.com/en_us/productpage.1008549003.html\n",
      "color: https://www2.hm.com/en_us/productpage.1008549004.html\n",
      "color: https://www2.hm.com/en_us/productpage.1008549006.html\n",
      "color: https://www2.hm.com/en_us/productpage.1008549007.html\n",
      "color: https://www2.hm.com/en_us/productpage.1008549008.html\n",
      "product: https://www2.hm.com/en_us/productpage.0979945003.html\n",
      "color: https://www2.hm.com/en_us/productpage.0979945001.html\n",
      "color: https://www2.hm.com/en_us/productpage.0979945002.html\n",
      "color: https://www2.hm.com/en_us/productpage.0979945003.html\n",
      "color: https://www2.hm.com/en_us/productpage.0979945009.html\n",
      "product: https://www2.hm.com/en_us/productpage.1024256002.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256001.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256002.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256003.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256004.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256005.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256006.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256007.html\n",
      "color: https://www2.hm.com/en_us/productpage.1024256008.html\n",
      "product: https://www2.hm.com/en_us/productpage.0979945001.html\n",
      "color: https://www2.hm.com/en_us/productpage.0979945001.html\n",
      "color: https://www2.hm.com/en_us/productpage.0979945002.html\n",
      "color: https://www2.hm.com/en_us/productpage.0979945003.html\n",
      "color: https://www2.hm.com/en_us/productpage.0979945009.html\n",
      "product: https://www2.hm.com/en_us/productpage.1008549006.html\n",
      "color: https://www2.hm.com/en_us/productpage.1008549001.html\n",
      "product: https://www2.hm.com/en_us/productpage.0690449022.html\n",
      "color: https://www2.hm.com/en_us/productpage.0690449001.html\n",
      "product: https://www2.hm.com/en_us/productpage.0979945002.html\n",
      "color: https://www2.hm.com/en_us/productpage.0979945001.html\n",
      "product: https://www2.hm.com/en_us/productpage.1013317006.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [81]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#url = 'https://www2.hm.com/en_us/productpage.0690449022.html' # test for one product\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Beautiful Soup\u001b[39;00m\n\u001b[0;32m     21\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mF:\\SamuelOliveiraAlvesd\\Anaconda3\\envs\\Scrapping_Jeans\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mF:\\SamuelOliveiraAlvesd\\Anaconda3\\envs\\Scrapping_Jeans\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mF:\\SamuelOliveiraAlvesd\\Anaconda3\\envs\\Scrapping_Jeans\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mF:\\SamuelOliveiraAlvesd\\Anaconda3\\envs\\Scrapping_Jeans\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mF:\\SamuelOliveiraAlvesd\\Anaconda3\\envs\\Scrapping_Jeans\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mF:\\SamuelOliveiraAlvesd\\Anaconda3\\envs\\Scrapping_Jeans\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\SamuelOliveiraAlvesd\\Anaconda3\\envs\\Scrapping_Jeans\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mF:\\SamuelOliveiraAlvesd\\Anaconda3\\envs\\Scrapping_Jeans\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mF:\\SamuelOliveiraAlvesd\\Anaconda3\\envs\\Scrapping_Jeans\\lib\\http\\client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mF:\\SamuelOliveiraAlvesd\\Anaconda3\\envs\\Scrapping_Jeans\\lib\\http\\client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mF:\\SamuelOliveiraAlvesd\\Anaconda3\\envs\\Scrapping_Jeans\\lib\\http\\client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mF:\\SamuelOliveiraAlvesd\\Anaconda3\\envs\\Scrapping_Jeans\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mF:\\SamuelOliveiraAlvesd\\Anaconda3\\envs\\Scrapping_Jeans\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mF:\\SamuelOliveiraAlvesd\\Anaconda3\\envs\\Scrapping_Jeans\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Empty dataframe\n",
    "df_compositions = pd.DataFrame()\n",
    "\n",
    "# unique columns for all products\n",
    "aux = []\n",
    "\n",
    "cols = {'Art. No.', 'Care instructions', 'Composition', 'Concept', 'Description', 'Fit', 'Imported',\n",
    " 'Material', 'More sustainable materials', 'Nice to know', 'Size', 'color_id', \n",
    " 'messages.clothingStyle', 'messages.garmentLength', 'messages.waistRise', 'style_id'}\n",
    "df_pattern = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    #Api Request\n",
    "    url = 'https://www2.hm.com/en_us/productpage.' + data.loc[i, 'product_id'] + '.html'\n",
    "    print(f'product: {url}')\n",
    "    #url = 'https://www2.hm.com/en_us/productpage.0690449022.html' # test for one product\n",
    "\n",
    "    page = requests.get(url, headers=headers)\n",
    "\n",
    "    # Beautiful Soup\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        # color name\n",
    "        product_list = soup.find_all('a', {'class':['filter-option miniature', 'filter-option miniature active']} )\n",
    "        color_name = [p.get('data-color') for p in product_list]\n",
    "\n",
    "        # product id\n",
    "        product_id = [p.get('data-articlecode') for p in product_list]\n",
    "\n",
    "        df_color = pd.DataFrame( [product_id, color_name]).T\n",
    "        df_color.columns = ['product_id', 'color_name']\n",
    "\n",
    "        #-------------------------- request detail products for colors -----------------------------\n",
    "        for j in range(len(df_color)):\n",
    "            #Api Request\n",
    "            url = 'https://www2.hm.com/en_us/productpage.' + df_color.loc[j, 'product_id'] + '.html'\n",
    "            print(f'color: {url}')\n",
    "\n",
    "            page = requests.get(url, headers=headers)\n",
    "\n",
    "            # Beautiful Soup\n",
    "            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "            size = soup.find_all('ul', class_='picker-list scrollable-list')\n",
    "        \n",
    "            # product name\n",
    "            products = soup.find_all('section', class_='product-name-price')\n",
    "            product_name = products[0].find('h1').get_text()\n",
    "\n",
    "            # price\n",
    "            product_price = products[0].find('span').get_text()\n",
    "            product_price = re.findall(r'\\d+\\.?\\d', product_price)[0]\n",
    "\n",
    "            #------------------------composition----------------------------\n",
    "\n",
    "            product_composition_list = soup.find_all('div', class_='details-attributes-list-item')\n",
    "            product_composition = [list(filter(None, p.get_text().split('\\n'))) for p in product_composition_list]\n",
    "\n",
    "            # rename dataframe\n",
    "            df_composition = pd.DataFrame(product_composition).T\n",
    "            df_composition.columns = df_composition.iloc[0]\n",
    "\n",
    "            # delete first row\n",
    "            df_composition = df_composition.iloc[1:].fillna(method='ffill')\n",
    "\n",
    "            # Remove shell, pocket and lining\n",
    "            df_composition['Composition'] = df_composition['Composition'].replace('Shell: ', '', regex=True)\n",
    "            df_composition['Composition'] = df_composition['Composition'].replace('Pocket lining: ', '', regex=True)\n",
    "            df_composition['Composition'] = df_composition['Composition'].replace('Pocket: ', '', regex=True)\n",
    "            df_composition['Composition'] = df_composition['Composition'].replace('Lining: ', '', regex=True)\n",
    "\n",
    "            # garantee the same number of columns\n",
    "            df_composition = pd.concat([df_pattern, df_composition], axis=0)\n",
    "\n",
    "            # keep new columns if they shows  up\n",
    "            aux = aux + df_composition.columns.tolist()\n",
    "\n",
    "            # collect most important columns\n",
    "            df_composition = df_composition[['Art. No.', 'Composition', 'Size', 'Fit', \n",
    "                                             'Material', 'Description']]\n",
    "\n",
    "            # rename columns\n",
    "            df_composition.columns = ['product_id', 'composition', 'size', 'fit', 'material', 'description']\n",
    "\n",
    "            # adding name and price\n",
    "            df_composition['product_name'] = product_name\n",
    "            df_composition['product_price'] = product_price\n",
    "\n",
    "            # merge data color + decomposition\n",
    "            df_composition = pd.merge(df_composition, df_color, how='left', on='product_id')\n",
    "\n",
    "            # all products\n",
    "            df_compositions = pd.concat([df_compositions, df_composition], axis=0)\n",
    "    except:\n",
    "        continue\n",
    "# Join showroom data + details\n",
    "df_compositions['style_id'] = df_compositions['product_id'].apply(lambda x: x[:-3])\n",
    "df_compositions['color_id'] = df_compositions['product_id'].apply(lambda x: x[-3:])\n",
    "\n",
    "\n",
    "# scrapy datetime\n",
    "df_compositions['scrapy_datetime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# saving scraping\n",
    "# df_compositions.to_csv('F:/SamuelOliveiraAlvesd/Desktop/Data_Science/Projetos/Ds_ao_Dev/webscraping_jeans/dataset/scraping_hm_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d3e9e5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.3 Limpeza dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c6ea7de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-05T14:52:50.080955Z",
     "start_time": "2022-08-05T14:52:49.904056Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make new copy\n",
    "#data1 = df_compositions.copy()\n",
    "\n",
    "# collect all collumns\n",
    "old_cols = ['product_id', 'composition', 'size', 'fit', 'material', 'description',\n",
    "       'product_name', 'product_price', 'color_name', 'style_id', 'color_id',\n",
    "       'scrapy_datetime']\n",
    "\n",
    "# function for new format for each collumn\n",
    "snakecase = lambda x : inflection.underscore(x)\n",
    "\n",
    "# applying change in columns for snakecase and underline\n",
    "new_cols = list(map(snakecase, old_cols))\n",
    "\n",
    "data1.columns = new_cols\n",
    "\n",
    "# Create Collumn Inner Leg\n",
    "data1['inner_leg_size'] = data1['size'].apply(lambda x: 89.5 if x=='Inner leg: Length: 89.5 cm (Size 33)'\n",
    "                                              else 0)\n",
    "\n",
    "# Create collumn Waist\n",
    "data1['waist_size'] = data1['size'].apply(lambda x: 90.5 if x=='Waist: Circumference: 90.5 cm (Size 33)'\n",
    "                                              else 0)\n",
    "\n",
    "# fillout size value\n",
    "data1['size'] = data1['size'].apply(lambda x: 0 if pd.isnull(x) else 33)\n",
    "\n",
    "# drop duplicates\n",
    "#data1 = data1.drop_duplicates()\n",
    "\n",
    "# product_name - undercase and removing special characters\n",
    "data1['product_name'] = data1['product_name'].apply(lambda x: x.replace(' ', '_').replace('®', '').lower())\n",
    "\n",
    "# color_name - undercase and removing space\n",
    "data1['color_name'] = data1['color_name'].apply(lambda x: x.replace(' ', '_').replace('/', '_').lower())\n",
    "\n",
    "# fit - undercase and removing space\n",
    "data1['fit'] = data1['fit'].apply(lambda x: x.replace(' ', '_').lower())\n",
    "\n",
    "#--------------Cleaning composition------------------------------------------------\n",
    "\n",
    "# break composition by comma\n",
    "dfaux = data1['composition'].str.split(',', expand=True).reset_index(drop=True)\n",
    "\n",
    "# choose order from columns\n",
    "df_ref = pd.DataFrame(index=np.arange(len(data1)), columns=['cotton', 'polyester', 'elastomultiester', 'spandex'])\n",
    "\n",
    "#--------------------------  cotton ----------------------------------------------\n",
    "# Collect cotton from first collumn\n",
    "df_cotton = dfaux.loc[dfaux[0].str.contains('Cotton', na=True), 0]\n",
    "df_cotton.name = 'cotton'\n",
    "\n",
    "df_ref = pd.concat([df_ref, df_cotton], axis=1)\n",
    "\n",
    "# Collect cotton from second collumn\n",
    "df_cotton2 = dfaux.loc[dfaux[1].str.contains('Cotton', na=True), 1]\n",
    "df_cotton2.name = 'cotton2'\n",
    "\n",
    "df_ref = pd.concat([df_ref, df_cotton2], axis=1)\n",
    "df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated(keep='last')]\n",
    "\n",
    "# merge results from cotton in one collumn\n",
    "df_ref['cotton'] = df_ref.apply(lambda x: x['cotton'] if pd.isna(x['cotton2']) else x['cotton2'], axis=1)\n",
    "df_ref.drop(columns='cotton2', inplace=True)\n",
    "\n",
    "#-------------------------- polyester ----------------------------------------------\n",
    "# Collect polyester from first collumn\n",
    "df_polyester = dfaux.loc[dfaux[0].str.contains('Polyester', na=True), 0]\n",
    "df_polyester.name = 'polyester'\n",
    "\n",
    "df_ref = pd.concat([df_ref, df_polyester], axis=1)\n",
    "\n",
    "# Collect polyester from second collumn\n",
    "df_polyester2 = dfaux.loc[dfaux[1].str.contains('Polyester', na=True), 1]\n",
    "df_polyester2.name = 'polyester2'\n",
    "\n",
    "df_ref = pd.concat([df_ref, df_polyester2], axis=1)\n",
    "df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated(keep='last')]\n",
    "\n",
    "# merge results from cotton in one collumn\n",
    "df_ref['polyester'] = df_ref.apply(lambda x: x['polyester'] if pd.isna(x['polyester2']) else x['polyester2'], axis=1)\n",
    "df_ref.drop(columns='polyester2', inplace=True)\n",
    "\n",
    "#-------------------------- Elastomultiester ----------------------------------------------\n",
    "# Collect elastomultiester from second collumn\n",
    "df_elastomultiester = dfaux.loc[dfaux[1].str.contains('Elastomultiester', na=True), 1]\n",
    "df_elastomultiester.name = 'elastomultiester'\n",
    "\n",
    "# merge results\n",
    "df_ref = pd.concat([df_ref, df_elastomultiester], axis=1)\n",
    "df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated(keep='last')]\n",
    "\n",
    "#-------------------------- spandex ----------------------------------------------\n",
    "# Collect spandex from second collumn\n",
    "df_spandex = dfaux.loc[dfaux[1].str.contains('Spandex', na=True), 1]\n",
    "df_spandex.name = 'spandex'\n",
    "\n",
    "df_ref = pd.concat([df_ref, df_spandex], axis=1)\n",
    "\n",
    "# Collect spandex from third collumn\n",
    "df_spandex2 = dfaux.loc[dfaux[2].str.contains('Spandex', na=True), 2]\n",
    "df_spandex2.name = 'spandex2'\n",
    "\n",
    "df_ref = pd.concat([df_ref, df_spandex2], axis=1)\n",
    "df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated(keep='last')]\n",
    "\n",
    "# merge results from cotton in one collumn\n",
    "df_ref['spandex'] = df_ref.apply(lambda x: x['spandex'] if pd.isna(x['spandex2']) else x['spandex2'], axis=1)\n",
    "df_ref.drop(columns='spandex2', inplace=True)\n",
    "\n",
    "# format composition data\n",
    "df_ref['cotton'] = df_ref['cotton'].apply(lambda x: int(re.search('\\d+', x).group(0))/100 if pd.notnull(x) else x)\n",
    "df_ref['polyester'] = df_ref['polyester'].apply(lambda x: int(re.search('\\d+', x).group(0))/100 if pd.notnull(x) else x)\n",
    "df_ref['elastomultiester'] = df_ref['elastomultiester'].apply(lambda x: int(re.search('\\d+', x).group(0))/100 if pd.notnull(x) else x)\n",
    "df_ref['spandex'] = df_ref['spandex'].apply(lambda x: int(re.search('\\d+', x).group(0))/100 if pd.notnull(x) else x)\n",
    "\n",
    "# drop all rows with NA in all collumns\n",
    "df_ref.dropna(axis=0, how='all', inplace=True)\n",
    "df_ref.fillna(0, inplace=True)\n",
    "\n",
    "# reset index and final join\n",
    "data1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data1 = pd.concat([data1, df_ref], axis=1)\n",
    "\n",
    "# description - undercase and removing space\n",
    "data1['description'] = data1['description'].apply(lambda x: x.replace(' ', '_').lower())\n",
    "\n",
    "# material - undercase and removing space\n",
    "data1['material'] = data1['material'].apply(lambda x: x.replace(' ', '_').lower())\n",
    "\n",
    "# product_price\n",
    "data1['product_price'] = data1['product_price'].astype('float64')\n",
    "\n",
    "# cotton\n",
    "data1['cotton'] = data1['cotton'].astype('float64')\n",
    "\n",
    "# polyester\n",
    "data1['polyester'] = data1['polyester'].astype('float64')\n",
    "\n",
    "# elastomultiester\n",
    "data1['elastomultiester'] = data1['elastomultiester'].astype('float64')\n",
    "\n",
    "# spandex\n",
    "data1['spandex'] = data1['spandex'].astype('float64')\n",
    "\n",
    "# drop composition\n",
    "data1.drop(columns='composition', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e8378",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.4 Insert into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37e19ff0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-03T19:48:42.598441Z",
     "start_time": "2022-08-03T19:48:42.585448Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# organize columns\n",
    "data1 = data1[['product_id', 'style_id', 'color_id', 'product_name', 'color_name','fit', 'size', 'inner_leg_size', 'waist_size',\n",
    "               'cotton', 'polyester', 'elastomultiester', 'spandex', 'material', 'description', 'scrapy_datetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c0102502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-05T18:17:00.387519Z",
     "start_time": "2022-08-05T18:17:00.368390Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create query from name of columns in database\n",
    "query_showroom_schema = \"\"\"\n",
    "    CREATE TABLE showroom(\n",
    "        product_id           TEXT,\n",
    "        style_id             TEXT,\n",
    "        color_id             TEXT,\n",
    "        product_name         TEXT,\n",
    "        color_name           TEXT,\n",
    "        fit                  TEXT,\n",
    "        size                 INTEGER,\n",
    "        inner_leg_size       REAL,\n",
    "        waist_size           REAL,\n",
    "        cotton               REAL,\n",
    "        polyester            REAL,\n",
    "        elastomultiester     REAL,\n",
    "        spandex              REAL,\n",
    "        material             TEXT,\n",
    "        description          TEXT,\n",
    "        scrapy_datetime      TEXT\n",
    "        )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ec869edd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-05T18:17:02.172741Z",
     "start_time": "2022-08-05T18:17:02.152382Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Connect and create an database\n",
    "conn = sqlite3.connect('hm_db.sqlite')\n",
    "\n",
    "# run query\n",
    "cursor = conn.execute(query_showroom_schema)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89357c77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-03T19:50:25.303987Z",
     "start_time": "2022-08-03T19:50:25.215999Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1708"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect in the database already create\n",
    "conn = create_engine('sqlite:///hm_db.sqlite', echo=False)\n",
    "\n",
    "# insert data to table\n",
    "data1.to_sql('showroom', con=conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5411c2",
   "metadata": {},
   "source": [
    "# 2.0 Descrição dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9b66b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-10T23:26:25.279738Z",
     "start_time": "2022-08-10T23:26:25.007777Z"
    }
   },
   "outputs": [],
   "source": [
    "path = 'F:/SamuelOliveiraAlvesd/Desktop/Data_Science/Projetos/Ds_ao_Dev/webscraping_jeans/'\n",
    "database_name = 'hm_db.sqlite'\n",
    "conn = create_engine('sqlite:///' + path + database_name, echo=False)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * FROM showroom\n",
    "\"\"\"\n",
    "\n",
    "df1 = pd.read_sql(query, con=conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6d42e",
   "metadata": {},
   "source": [
    "## 2.1 Dimensão dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6c0d578",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-10T23:26:26.425251Z",
     "start_time": "2022-08-10T23:26:26.415256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 20349\n",
      "Number of cols: 16\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of rows: {df1.shape[0]}')\n",
    "print(f'Number of cols: {df1.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0bffd",
   "metadata": {},
   "source": [
    "## 2.2 Tipo dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4da7f26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-10T23:26:27.497834Z",
     "start_time": "2022-08-10T23:26:27.463862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id                  object\n",
       "style_id                    object\n",
       "color_id                    object\n",
       "product_name                object\n",
       "color_name                  object\n",
       "fit                         object\n",
       "size                         int64\n",
       "inner_leg_size             float64\n",
       "waist_size                 float64\n",
       "cotton                     float64\n",
       "polyester                  float64\n",
       "elastomultiester           float64\n",
       "spandex                    float64\n",
       "material                    object\n",
       "description                 object\n",
       "scrapy_datetime     datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert date from object to date time\n",
    "df1['scrapy_datetime'] = pd.to_datetime(df1['scrapy_datetime'])\n",
    "\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7d7c7",
   "metadata": {},
   "source": [
    "## 2.3 Identificação dos dados faltantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "866b2f2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-10T23:26:28.803376Z",
     "start_time": "2022-08-10T23:26:28.768396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id          0\n",
       "style_id            0\n",
       "color_id            0\n",
       "product_name        0\n",
       "color_name          0\n",
       "fit                 0\n",
       "size                0\n",
       "inner_leg_size      0\n",
       "waist_size          0\n",
       "cotton              0\n",
       "polyester           0\n",
       "elastomultiester    0\n",
       "spandex             0\n",
       "material            0\n",
       "description         0\n",
       "scrapy_datetime     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61aa35c",
   "metadata": {},
   "source": [
    "## 2.4 Descrição dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e1385f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-10T23:26:30.124710Z",
     "start_time": "2022-08-10T23:26:30.119713Z"
    }
   },
   "outputs": [],
   "source": [
    "num_attributes = df1.select_dtypes(include=['int64', 'float64'])\n",
    "cat_attributes = df1.select_dtypes(exclude=['int64', 'float64', 'datetime64[ns]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e82cb",
   "metadata": {},
   "source": [
    "### 2.5.1 Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a382a243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-10T23:26:31.563919Z",
     "start_time": "2022-08-10T23:26:31.519934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attibutes</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>range</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>size</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>6.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.94</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inner_leg_size</td>\n",
       "      <td>0.00</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.42</td>\n",
       "      <td>6.36</td>\n",
       "      <td>38.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>waist_size</td>\n",
       "      <td>0.00</td>\n",
       "      <td>90.50</td>\n",
       "      <td>90.50</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.53</td>\n",
       "      <td>2.19</td>\n",
       "      <td>2.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cotton</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>polyester</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-1.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>elastomultiester</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>11.58</td>\n",
       "      <td>132.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spandex</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.21</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          attibutes  min   max  range  mean  median   std  skew  kurtosis\n",
       "0              size 0.00 33.00  33.00  6.27    0.00 12.94  1.58      0.50\n",
       "1    inner_leg_size 0.00 89.50  89.50  2.06    0.00 13.42  6.36     38.51\n",
       "2        waist_size 0.00 90.50  90.50 11.85    0.00 30.53  2.19      2.79\n",
       "3            cotton 0.00  1.00   1.00  0.72    0.98  0.32 -0.48     -1.57\n",
       "4         polyester 0.00  1.00   1.00  0.27    0.00  0.32  0.48     -1.58\n",
       "5  elastomultiester 0.00  0.09   0.09  0.00    0.00  0.01 11.58    132.42\n",
       "6           spandex 0.00  0.02   0.02  0.00    0.00  0.01  1.21     -0.03"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Central tendency and Dispersion\n",
    "d1 = pd.DataFrame (num_attributes.apply( lambda x: x.max() - x.min() )).T\n",
    "d2 = num_attributes.agg(['min', 'max', 'mean', 'median', 'std', 'skew', 'kurtosis'])\n",
    "\n",
    "# concatenate\n",
    "m2 = pd.concat([d1, d2]).T.reset_index()\n",
    "m2.columns = ['attibutes', 'range', 'min', 'max', 'mean', 'median', 'std', 'skew', 'kurtosis']\n",
    "m2 = m2[['attibutes', 'min', 'max', 'range', 'mean', 'median', 'std', 'skew', 'kurtosis']]\n",
    "\n",
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff49ec",
   "metadata": {},
   "source": [
    "### 2.5.2 Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f468dc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-10T23:26:42.023136Z",
     "start_time": "2022-08-10T23:26:41.985148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id      139\n",
       "style_id         20\n",
       "color_id         39\n",
       "product_name     14\n",
       "color_name       28\n",
       "fit               5\n",
       "material          2\n",
       "description      20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_attributes.apply( lambda x: x.unique().shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
